{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8f70269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Append the parent directory to sys.path using os.getcwd() instead of __file__\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from collections import deque\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "import ollama\n",
    "\n",
    "from src.utils.file_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40198a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_LLM_with_JSON(prompt, ollama_client, model, model_options):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant, who helps the user with their query. You are designed to output JSON.\"})     \n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})   \n",
    "\n",
    "    response = ollama_client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options=model_options\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def recover_json(json_str, verbose = False):\n",
    "    decoded_object = {}\n",
    "\n",
    "    if '{' not in json_str:\n",
    "        return json_str\n",
    "\n",
    "    json_str = extract_json(json_str)\n",
    "\n",
    "    try:\n",
    "        decoded_object = json.loads(json_str)\n",
    "        return decoded_object\n",
    "    except Exception:\n",
    "        try:\n",
    "            decoded_object = json.loads(json_str.replace(\"'\", '\"'))\n",
    "            return decoded_object\n",
    "        except Exception:\n",
    "            try:\n",
    "                decoded_object = json_repair.loads(json_str.replace(\"'\", '\"'))\n",
    "\n",
    "                for k, d in decoded_object.items():\n",
    "                    dd = d.replace(\"'\", '\"')\n",
    "                    decoded_object[k] = json.loads(dd)\n",
    "                \n",
    "                return decoded_object\n",
    "            except:\n",
    "                print(f\"all json recovery operations have failed for {json_str}\")\n",
    "        \n",
    "            if verbose:\n",
    "                if isinstance(decoded_object, dict):\n",
    "                    print(f\"\\n{bc.OKBLUE}>>> Recovering JSON:\\n{bc.OKGREEN}{json.dumps(decoded_object, indent=3)}{bc.ENDC}\")\n",
    "                else:\n",
    "                    print(f\"\\n{bc.OKBLUE}>>> Recovering JSON:\\n{bc.OKGREEN}{json_str}{bc.ENDC}\")\n",
    "\n",
    "\n",
    "    return json_str\n",
    "\n",
    "def extract_json(s):\n",
    "    code = re.search(r\"```json(.*?)```\", s, re.DOTALL)\n",
    "    if code:\n",
    "        return code.group(1)\n",
    "    else:\n",
    "        return s\n",
    "    \n",
    "def chunk_markdown_table_with_overlap(md_table, cols=None, n_tokens=512, overlap=128):\n",
    "    \"\"\"\n",
    "    Splits a markdown table into chunks with overlapping tokens.\n",
    "    Each returned chunk already has the header prepended.\n",
    "\n",
    "    Parameters:\n",
    "        - md_table (str): The markdown table as a string.\n",
    "        - cols (list of str], optional): List of column headers. If provided, overrides the header in md_table.\n",
    "        - n_tokens (int): Maximum number of tokens per chunk (including header tokens).\n",
    "        - overlap (int): Number of tokens to overlap between adjacent chunks (data‐rows only).\n",
    "\n",
    "    Returns:\n",
    "        - chunks (list of str): Each element is a string consisting of\n",
    "                                (header + data‐rows with overlaps).\n",
    "        - header (str): The header block (header row + separator row).\n",
    "    \"\"\"\n",
    "    # Split into non‐empty, stripped lines\n",
    "    mds = [line.rstrip() for line in md_table.strip().split(\"\\n\") if line.strip()]\n",
    "    if not mds:\n",
    "        return [], \"\"\n",
    "\n",
    "    # Build header based on cols override or the first two lines of md_table\n",
    "    if cols is not None:\n",
    "        # Use the provided cols to generate a new header\n",
    "        header_row = \"| \" + \" | \".join(cols) + \" |\"\n",
    "        separator = \"| \" + \" | \".join([\"---\"] * len(cols)) + \" |\"\n",
    "        header = header_row + \"\\n\" + separator + \"\\n\"\n",
    "        # Skip the first two lines of mds (the original header + separator)\n",
    "        data_start_idx = 2\n",
    "    else:\n",
    "        # Use the existing first line as header; check if second line is the separator\n",
    "        header = mds[0] + \"\\n\"\n",
    "        if len(mds) > 1 and all(ch in \"-:|\" for ch in mds[1].replace(\" \", \"\")):\n",
    "            header += mds[1] + \"\\n\"\n",
    "            data_start_idx = 2\n",
    "        else:\n",
    "            data_start_idx = 1\n",
    "\n",
    "    # Count how many tokens the header takes (so each chunk can include that cost)\n",
    "    header_token_count = get_token_count(header)\n",
    "\n",
    "    chunks = []                  # Will hold final strings: (header + data‐rows)\n",
    "    current_chunk_data = []      # Collects just the data‐rows (each ending in \"\\n\")\n",
    "    current_token_count = header_token_count\n",
    "\n",
    "    # Iterate over each data row from the markdown table\n",
    "    for raw_row in mds[data_start_idx:]:\n",
    "        row = raw_row.rstrip()\n",
    "        if not row.startswith(\"|\"):\n",
    "            # Skip lines that aren't table rows\n",
    "            continue\n",
    "        row_with_nl = row + \"\\n\"\n",
    "        row_tokens = get_token_count(row_with_nl)\n",
    "\n",
    "        # If adding this row would exceed n_tokens, finalize the current chunk\n",
    "        if current_token_count + row_tokens > n_tokens:\n",
    "            # Prepend the header to the collected rows and append to chunks\n",
    "            chunk_text = header + \"\".join(current_chunk_data)\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "            # Build overlap buffer by walking backwards over the data rows\n",
    "            overlap_buffer = []\n",
    "            overlap_tokens = 0\n",
    "            for prev_row_with_nl in reversed(current_chunk_data):\n",
    "                prev_tokens = get_token_count(prev_row_with_nl)\n",
    "                if overlap_tokens + prev_tokens > overlap:\n",
    "                    break\n",
    "                overlap_buffer.insert(0, prev_row_with_nl)\n",
    "                overlap_tokens += prev_tokens\n",
    "\n",
    "            # Start a new chunk with just the overlap rows\n",
    "            current_chunk_data = list(overlap_buffer)\n",
    "            current_token_count = header_token_count + overlap_tokens\n",
    "\n",
    "        # Add this row to the (new or continuing) chunk\n",
    "        current_chunk_data.append(row_with_nl)\n",
    "        current_token_count += row_tokens\n",
    "\n",
    "    # If any rows remain, append them as the final chunk\n",
    "    if current_chunk_data:\n",
    "        chunk_text = header + \"\".join(current_chunk_data)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks, header\n",
    "\n",
    "# def chunk_markdown_tables(\n",
    "#     tables_dir: str,\n",
    "#     tables_summaries_dir: str,\n",
    "#     prompt: str,\n",
    "#     cols: List[List[str]],\n",
    "#     n_tokens: int,\n",
    "#     overlap: int,\n",
    "#     ollama_client: ollama_client,\n",
    "#     model: str,\n",
    "#     model_options: Dict,\n",
    "#     tables_chunks_dir: str\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Opens markdown tables and split them into chunnks, then, stored the resulting tables chunks as well as summaries in dedicated folders.\n",
    "\n",
    "#     Args:\n",
    "#         - tables_dir (str): The path to the directory where to find the tables.\n",
    "#         - tables_summaries_dir (str): The path to the directory where to find the tables.\n",
    "#         - prompt (str): The prompt to use for the model.\n",
    "#         - cols (List(List(str))): A list of lists of columns corresponding  to the tables respectively.\n",
    "#         - n_token (int): The chunk size.\n",
    "#         - overlap (int): Number of tokens to overlap between adjacent chunks (data‐rows only).\n",
    "#         - model (str): The name of the model to use for chunking.\n",
    "#         - ollama_client (Ollama Client): The Ollama client to use for querying the model.\n",
    "#         - model_options (Dict): A dictionary containaing the options (like the temperature) to run the model.\n",
    "#         - tables_chunks_dir (str): The path to the directory where to stored the generated tables chunks.\n",
    "\n",
    "#     returns:\n",
    "#         None.\n",
    "#     \"\"\"\n",
    "#     tables_path = Path(tables_dir)\n",
    "#     tables_summaries_path = Path(tables_summaries_dir)\n",
    "#     tables_chunks_path = Path(tables_chunks_dir)\n",
    "\n",
    "#     # tables_path.mkdir(parents=True, exist_ok=True)\n",
    "#     tables_summaries_path.mkdir(parents=True, exist_ok=True)\n",
    "#     tables_chunks_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     if not tables_path.is_dir():\n",
    "#         raise ValueError(f\"Folder '{tables_path}' does not exist.\")\n",
    "\n",
    "#     # if not tables_path.is_dir() or not any(tables_path.iterdir()):\n",
    "#     #     raise ValueError(f\"Folder '{tables_path}' does not exist or is empty.\")\n",
    "\n",
    "#     print(f\"\\nProcessing the tables in {tables_path}...\\n\")\n",
    "\n",
    "#     if not cols:\n",
    "#         for table_path in tables_path.rglob(\"*.md\"):\n",
    "#             if table_path.is_file():\n",
    "#                 print(f\"\\nReading file: {table_path}\")\n",
    "#                 with open(table_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                     md_table = f.read()\n",
    "#                     chunks, _, summary = chunk_markdown_table(\n",
    "#                         prompt, md_table, cols, n_tokens, overlap, ollama_client, model, model_options\n",
    "#                     )\n",
    "\n",
    "#                     sumary_file_name = (\n",
    "#                         tables_summaries_path / f\"{table_path.stem}_summary.txt\"\n",
    "#                     )\n",
    "#                     with open(sumary_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "#                         file.write(summary)\n",
    "#                     print(f\"Saved table summary to: {sumary_file_name}\")\n",
    "\n",
    "#                     for idx, chunk in enumerate(chunks, 1):\n",
    "#                         chunk_with_summary = f\"{summary}\\n\\n{chunk}\"\n",
    "#                         table_name = (\n",
    "#                             tables_chunks_path / f\"{table_path.stem}_chunk_{idx}.md\"\n",
    "#                         )\n",
    "#                         with open(table_name, \"w\", encoding=\"utf-8\") as file:\n",
    "#                             file.write(chunk_with_summary)\n",
    "#                         print(f\"Saved table chunk to: {table_name}\")\n",
    "\n",
    "#     elif len(cols) == len(list(tables_path.rglob(\"*.md\"))):\n",
    "#         for idx, table_path in enumerate(tables_path.rglob(\"*.md\")):\n",
    "#             if table_path.is_file():\n",
    "#                 print(f\"\\nReading file: {table_path}\")\n",
    "#                 with open(table_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                     md_table = f.read()\n",
    "#                     chunks, _, summary = chunk_markdown_table(\n",
    "#                         prompt, md_table, cols[idx], n_tokens, overlap, ollama_client, model, model_options\n",
    "#                     )\n",
    "#                     sumary_file_name = (\n",
    "#                         tables_summaries_path / f\"{table_path.stem}_summary.txt\"\n",
    "#                     )\n",
    "#                     with open(sumary_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "#                         file.write(summary)\n",
    "#                     print(f\"Saved table summary to: {sumary_file_name}\")\n",
    "\n",
    "#                     for idxx, chunk in enumerate(chunks, 1):\n",
    "#                         table_name = (\n",
    "#                             tables_chunks_path / f\"{table_path.stem}_chunk_{idxx}.md\"\n",
    "#                         )\n",
    "#                         with open(table_name, \"w\", encoding=\"utf-8\") as file:\n",
    "#                             file.write(chunk)\n",
    "#                         print(f\"Saved table to: {table_name}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"Please provide a list of columns for each table.\")\n",
    "\n",
    "#     print(\n",
    "#         f\"\\nAll tables have been processed.\\nTables chunks were saved in Markdown to: {tables_dir}.\\nTable summaries were saved in TXT to: {tables_summaries_dir}.\\n\"\n",
    "#     )\n",
    "\n",
    "def chunk_markdown_table(prompt, md_table, cols, n_tokens, overlap, ollama_client, model, model_options):\n",
    "    prompt = prompt.format(\n",
    "        table=md_table.split(\"\\n\")\n",
    "    )\n",
    "    output = ask_LLM_with_JSON(prompt, ollama_client, model, model_options)\n",
    "    try:\n",
    "        outd = recover_json(output)\n",
    "        cols = outd[\"columns\"].split(\",\")\n",
    "        summary = outd[\"summary_of_the_table\"]\n",
    "    except:\n",
    "        logc(f\"Could not recover with malformed JSON {output}\")\n",
    "        return [], \"\", \"\"\n",
    "\n",
    "    chunks, header = chunk_markdown_table_with_overlap(\n",
    "        md_table, cols, n_tokens=n_tokens, overlap=overlap\n",
    "    )\n",
    "    print(\"Chunks:\", len(chunks))\n",
    "\n",
    "    return chunks, header, summary\n",
    "\n",
    "def get_token_count(input_text: str) -> int:\n",
    "    \"\"\"Returns the number of tokens for the input text.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(input_text))\n",
    "\n",
    "def hard_split(text, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Fallback simple splitter that breaks a text into chunks\n",
    "    of at most max_chunk_size tokens, using whitespace as delimiter.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if not current:\n",
    "            current = word\n",
    "        else:\n",
    "            candidate = f\"{current} {word}\"\n",
    "            # If adding this word would exceed max_chunk_size, finalize current chunk\n",
    "            if get_token_count(candidate) > max_chunk_size:\n",
    "                chunks.append(current)\n",
    "                current = word\n",
    "            else:\n",
    "                current = candidate\n",
    "\n",
    "    # Add the last chunk if nonempty\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_semantic_split(texts, semantic_splitter, max_chunk_size):\n",
    "    \"\"\"Splits a list of texts semantically into chunks within the token limit.\"\"\"\n",
    "    # Initialize the queue and result list\n",
    "    queue = deque(texts)\n",
    "    result = []\n",
    "    step = 0\n",
    "\n",
    "    while queue:\n",
    "        step += 1\n",
    "        message = f\"\\nProcessing step {step} with {len(queue)} segments in the queue.\"\n",
    "        print(message)\n",
    "        print(\"=\" * len(message))\n",
    "        # Pop the next segment from the queue\n",
    "        segment = queue.popleft()\n",
    "        print(f\"\\nCurrent segment: {segment[:10]}... (length: {get_token_count(segment)} tokens)\")\n",
    "\n",
    "        # Skip empty or whitespace-only segments\n",
    "        if not segment.strip():\n",
    "            print(\"\\nSkipping empty segment.\")\n",
    "            continue\n",
    "\n",
    "        # Compute token count once per segment\n",
    "        token_count = get_token_count(segment)\n",
    "        if token_count == 0:\n",
    "            print(\"\\nSkipping segment with zero token count.\")\n",
    "            continue\n",
    "\n",
    "        # If the segment exceeds max size, attempt semantic splitting\n",
    "        if token_count > max_chunk_size:\n",
    "            print(f\"\\nSegment exceeds max chunk size of {max_chunk_size} tokens (has {token_count}).\")\n",
    "            sub_segments = semantic_splitter.split_text(segment)\n",
    "\n",
    "            # If no valid sub-segments were created, fallback to hard split\n",
    "            if not sub_segments or sub_segments == [segment]:\n",
    "                print(\"\\nNo semantic chunks created; using hard split fallback.\")\n",
    "                fallback_chunks = hard_split(segment, max_chunk_size)\n",
    "                for fc in fallback_chunks:\n",
    "                    print(f\"  - Adding fallback chunk (tokens: {get_token_count(fc)}): {fc[:30]}...\")\n",
    "                    result.append(fc)\n",
    "                continue\n",
    "\n",
    "            # Reinsert sub-segments at the front of the queue in reverse order\n",
    "            for sub in reversed(sub_segments):\n",
    "                queue.appendleft(sub)\n",
    "            continue\n",
    "\n",
    "        # If the segment is within the token limit, add it to the result\n",
    "        print(f\"\\nSegment is within max chunk size of {max_chunk_size} tokens (has {token_count}).\")\n",
    "        result.append(segment)\n",
    "\n",
    "    return result\n",
    "\n",
    "# def semantic_chunk_text_file(\n",
    "#     file_path: Path,\n",
    "#     embed_model_info: Dict,\n",
    "#     max_chunk_size: int,\n",
    "#     buffer_size: int,\n",
    "#     breakpoint_threshold_type: str,\n",
    "#     breakpoint_threshold_amount: float,\n",
    "#     sentence_split_regex: str,\n",
    "#     verbose: bool,\n",
    "# ):\n",
    "#     \"\"\"Processes a single text file and returns semantic chunks.\"\"\"\n",
    "#     # Initialize embeddings and splitters\n",
    "#     embedding_model = AzureOpenAIEmbeddings(\n",
    "#         azure_deployment=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "#         azure_endpoint=f\"https://{embed_model_info['AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE']}.openai.azure.com\",\n",
    "#         openai_api_key=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE_KEY\"],\n",
    "#         openai_api_version=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL_API_VERSION\"],\n",
    "#     )\n",
    "\n",
    "#     semantic_splitter = SemanticChunker(\n",
    "#         embedding_model,\n",
    "#         buffer_size=buffer_size,\n",
    "#         breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "#         breakpoint_threshold_amount=breakpoint_threshold_amount,\n",
    "#         sentence_split_regex=sentence_split_regex,\n",
    "#     )\n",
    "\n",
    "#     # headers_to_split_on = [\n",
    "#     #     (\"#\", \"header_1\"),\n",
    "#     #     (\"##\", \"header_2\"),\n",
    "#     #     (\"###\", \"header_3\"),\n",
    "#     # ]\n",
    "#     # markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "#     #     headers_to_split_on=headers_to_split_on,\n",
    "#     #     strip_headers=False\n",
    "#     # )\n",
    "\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         input_text = f.read()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\nReading file: {file_path}\")\n",
    "\n",
    "#     # Markdown split\n",
    "#     # md_header_splits = markdown_splitter.split_text(input_text)\n",
    "#     # plain_content = [value.page_content for value in md_header_splits]\n",
    "\n",
    "#     # Semantic split\n",
    "#     semantic_chunks = process_semantic_split(\n",
    "#         [input_text], semantic_splitter, max_chunk_size\n",
    "#     )\n",
    "#     # semantic_chunks = process_semantic_split(plain_content, semantic_splitter, max_chunk_size)\n",
    "\n",
    "#     return semantic_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96085c64",
   "metadata": {},
   "source": [
    "### Chunk Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7d3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_PATH = \"../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6a60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [os.path.join(DOCS_PATH, d) for d in os.listdir(DOCS_PATH)]\n",
    "tables = [d for d in docs if d.endswith(\".md\") and \"_Table_\" in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76a66ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/46- Unit of Activity (UoA)_page_1_Table_2.md',\n",
       " '../data/processed/46- Unit of Activity (UoA)_page_2_Table_1.md',\n",
       " '../data/processed/46- Unit of Activity (UoA)_page_1_Table_1.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820ef13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tables[1], \"r\", encoding=\"utf-8\") as f:\n",
    "    md_table = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454585a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| HOB/Business name   | Pacesetter   | CHJV                            | PetroAlliance         | Slider                   | Radius Motors            | Radius DTSS               | PAWC-Cementing                   |\n",
      "|---------------------|--------------|---------------------------------|-----------------------|--------------------------|--------------------------|---------------------------|----------------------------------|\n",
      "| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |\n",
      "| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |\n",
      "| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |\n",
      "| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |\n"
     ]
    }
   ],
   "source": [
    "print(md_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c181f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Data Engineer resonsible for reforming and preserving the quality of Markdown tables. A table will be passed to you in the form of a Markdown string. You are designed to output JSON. \n",
      "\n",
      "Your task is to extract the column names of the header of the table from the Markdown string in the form of a comma-separated list. If the column names do exist, please return them verbatim word-for-word with no change, except fixing format or alignment issues (extra spaces and new lines can be removed). \n",
      "\n",
      "If the table does not have a header, then please check the data rows and generate column names for the header that fit the data types of the columns and the nature of the data. \n",
      "\n",
      "**VERY IMPORTANT**: If the table has an unnamed index column, typically the leftmost column, you **MUST** generate a column name for it.\n",
      "\n",
      "Finally, please generate a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be to the point with two or three paragraphs.\n",
      "\n",
      "The Markdown table: \n",
      "## START OF MARKDOWN TABLE\n",
      "{table}\n",
      "## END OF MARKDOWN TABLE\n",
      "\n",
      "JSON OUTPUT:\n",
      "You **MUST** generate the below JSON dictionary as your output. \n",
      "\n",
      "{{\n",
      "    \"columns\": \"list of comma-separated column names. If the table has a header, please return the column names as they are. If the table does not have a header, then generate column names that fit the data types and nature of the data. Do **NOT** forget any unnamed index columns.\",\n",
      "    \"columns_inferred\": \"true/false. Set to true in the case the table does not have a header, and you generated column names based on the data rows.\",\n",
      "    \"total_number_of_columns\": \"total number of columns in the table\",\n",
      "    \"summary_of_the_table\": \"a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be concise and to the point, one or two short paragraphs.\"\n",
      "}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/prompts/markdown_extract_header_and_summarize_prompt.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927dc7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Data Engineer resonsible for reforming and preserving the quality of Markdown tables. A table will be passed to you in the form of a Markdown string. You are designed to output JSON. \n",
      "\n",
      "Your task is to extract the column names of the header of the table from the Markdown string in the form of a comma-separated list. If the column names do exist, please return them verbatim word-for-word with no change, except fixing format or alignment issues (extra spaces and new lines can be removed). \n",
      "\n",
      "If the table does not have a header, then please check the data rows and generate column names for the header that fit the data types of the columns and the nature of the data. \n",
      "\n",
      "**VERY IMPORTANT**: If the table has an unnamed index column, typically the leftmost column, you **MUST** generate a column name for it.\n",
      "\n",
      "Finally, please generate a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be to the point with two or three paragraphs.\n",
      "\n",
      "The Markdown table: \n",
      "## START OF MARKDOWN TABLE\n",
      "['| HOB/Business name   | Pacesetter   | CHJV                            | PetroAlliance         | Slider                   | Radius Motors            | Radius DTSS               | PAWC-Cementing                   |', '|---------------------|--------------|---------------------------------|-----------------------|--------------------------|--------------------------|---------------------------|----------------------------------|', '| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |', '| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |', '| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |', '| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |', '| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |', '| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |']\n",
      "## END OF MARKDOWN TABLE\n",
      "\n",
      "JSON OUTPUT:\n",
      "You **MUST** generate the below JSON dictionary as your output. \n",
      "\n",
      "{\n",
      "    \"columns\": \"list of comma-separated column names. If the table has a header, please return the column names as they are. If the table does not have a header, then generate column names that fit the data types and nature of the data. Do **NOT** forget any unnamed index columns.\",\n",
      "    \"columns_inferred\": \"true/false. Set to true in the case the table does not have a header, and you generated column names based on the data rows.\",\n",
      "    \"total_number_of_columns\": \"total number of columns in the table\",\n",
      "    \"summary_of_the_table\": \"a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be concise and to the point, one or two short paragraphs.\"\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt= prompt.format(\n",
    "    table=md_table.split(\"\\n\")[:100]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7cb192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = ollama.Client()\n",
    "model = \"phi4\"\n",
    "model_options = {\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": .95\n",
    "}\n",
    "\n",
    "json_output = ask_LLM_with_JSON(prompt, ollama_client, model, model_options)\n",
    "json_output = recover_json(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72c6c99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c3c2490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': 'HOB/Business name, Pacesetter, CHJV, PetroAlliance, Slider, Radius Motors, Radius DTSS, PAWC-Cementing',\n",
       " 'columns_inferred': False,\n",
       " 'total_number_of_columns': 8,\n",
       " 'summary_of_the_table': \"The table provides a structured overview of various business units and their associated operational metrics or roles within an organization. It lists different business names alongside specific performance indicators or managerial responsibilities, such as 'WCM', 'DMHV', and 'CHG'. The columns represent distinct categories like 'Pacesetter', 'CHJV', and others, which likely correspond to key performance areas or strategic initiatives. Additionally, the table includes information on resource management roles, indicating who is responsible for certain operational aspects within these business units. This setup suggests a focus on tracking performance metrics and managerial oversight across different segments of the organization.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6f270a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The table provides a structured overview of various business units and their associated operational metrics or roles within an organization. It lists different business names alongside specific performance indicators or managerial responsibilities, such as 'WCM', 'DMHV', and 'CHG'. The columns represent distinct categories like 'Pacesetter', 'CHJV', and others, which likely correspond to key performance areas or strategic initiatives. Additionally, the table includes information on resource management roles, indicating who is responsible for certain operational aspects within these business units. This setup suggests a focus on tracking performance metrics and managerial oversight across different segments of the organization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(json_output[\"summary_of_the_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26671b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOB/Business name',\n",
       " ' Pacesetter',\n",
       " ' CHJV',\n",
       " ' PetroAlliance',\n",
       " ' Slider',\n",
       " ' Radius Motors',\n",
       " ' Radius DTSS',\n",
       " ' PAWC-Cementing']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = json_output[\"columns\"].split(\",\")\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86b033c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, header = chunk_markdown_table_with_overlap(md_table, cols=None, n_tokens=512, overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f9f30ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "| HOB/Business name   | Pacesetter   | CHJV                            | PetroAlliance         | Slider                   | Radius Motors            | Radius DTSS               | PAWC-Cementing                   |\n",
      "|---------------------|--------------|---------------------------------|-----------------------|--------------------------|--------------------------|---------------------------|----------------------------------|\n",
      "| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |\n",
      "| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |\n",
      "| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |\n",
      "| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| HOB/Business name   | Pacesetter   | CHJV                            | PetroAlliance         | Slider                   | Radius Motors            | Radius DTSS               | PAWC-Cementing                   |\n",
       "|---------------------|--------------|---------------------------------|-----------------------|--------------------------|--------------------------|---------------------------|----------------------------------|\n",
       "| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |\n",
       "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |\n",
       "| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |\n",
       "| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |\n",
       "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |\n",
       "| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk}\\n\\n\")\n",
    "    display(Markdown(chunk))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "02863cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Data Engineer resonsible for reforming and preserving the quality of Markdown tables. A table will be passed to you in the form of a Markdown string. You are designed to output JSON. \n",
      "\n",
      "Your task is to extract the column names of the header of the table from the Markdown string in the form of a comma-separated list. If the column names do exist, please return them verbatim word-for-word with no change, except fixing format or alignment issues (extra spaces and new lines can be removed). \n",
      "\n",
      "If the table does not have a header, then please check the data rows and generate column names for the header that fit the data types of the columns and the nature of the data. \n",
      "\n",
      "**VERY IMPORTANT**: If the table has an unnamed index column, typically the leftmost column, you **MUST** generate a column name for it.\n",
      "\n",
      "Finally, please generate a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be to the point with two or three paragraphs.\n",
      "\n",
      "The Markdown table: \n",
      "## START OF MARKDOWN TABLE\n",
      "{table}\n",
      "## END OF MARKDOWN TABLE\n",
      "\n",
      "JSON OUTPUT:\n",
      "You **MUST** generate the below JSON dictionary as your output. \n",
      "\n",
      "{{\n",
      "    \"columns\": \"list of comma-separated column names. If the table has a header, please return the column names as they are. If the table does not have a header, then generate column names that fit the data types and nature of the data. Do **NOT** forget any unnamed index columns.\",\n",
      "    \"columns_inferred\": \"true/false. Set to true in the case the table does not have a header, and you generated column names based on the data rows.\",\n",
      "    \"total_number_of_columns\": \"total number of columns in the table\",\n",
      "    \"summary_of_the_table\": \"a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be concise and to the point, one or two short paragraphs.\"\n",
      "}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/prompts/markdown_extract_header_and_summarize_prompt.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab53c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "cols=None\n",
    "n_tokens=512\n",
    "overlap=128\n",
    "chunks, header, summary = chunk_markdown_table(prompt, md_table, cols, n_tokens, overlap, ollama_client, model, model_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2cfd527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| HOB/Business name |  Pacesetter |  CHJV |  PetroAlliance |  Slider |  Radius Motors |  Radius DTSS |  PAWC-Cementing |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d95df609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "| HOB/Business name |  Pacesetter |  CHJV |  PetroAlliance |  Slider |  Radius Motors |  Radius DTSS |  PAWC-Cementing |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |\n",
      "| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |\n",
      "| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |\n",
      "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |\n",
      "| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| HOB/Business name |  Pacesetter |  CHJV |  PetroAlliance |  Slider |  Radius Motors |  Radius DTSS |  PAWC-Cementing |\n",
       "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
       "| UoA Business Line   | WCM          | WCM                             | WCM                   | WCD                      | WCD                      | WCF                       | UoA Sub-Business Line            |\n",
       "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | UoA Geounit                      |\n",
       "| CAL                 | CHG          | RUL                             | USL                   | RUL                      | RUL                      | RUL                       | UoA Location                     |\n",
       "| PKST                | CHJV         | PAD                             | PSL                   | n/a                      | n/a                      | PAWC                      | Revenue recognized under         |\n",
       "| PCSB                | DMHV         | DMHV                            | SLID                  | MOT                      | DTSS                     | WIT                       | POC responsible for manual input |\n",
       "| PCSB Controller     | CHG WEC PSCM | PetroAlliance Resources Manager | USL Resources Manager | Radius Resources Manager | Radius Resources Manager | WIT Resources Coordinator | Controlled                       |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk}\\n\\n\")\n",
    "    display(Markdown(chunk))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d4338d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The table provides a structured overview of various business lines and their associated operational units within an organization. It details the alignment between different business names (HOB/Business name) and specific operational metrics or roles such as Pacesetter, CHJV, PetroAlliance, Slider, Radius Motors, Radius DTSS, and PAWC-Cementing. Each row represents a unique combination of these elements, indicating how various units like PCSB, CAL, PKST, and others are managed under different operational frameworks (e.g., WCM, DMHV). The table also highlights the roles responsible for manual input or control within these business lines, such as 'POC responsible for manual input' and 'Controlled,' suggesting a focus on accountability and resource management. Overall, this table serves to map out organizational structures and responsibilities across different operational units."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5026e",
   "metadata": {},
   "source": [
    "### Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35b833de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 text files.\n",
      "\n",
      "Texts:\n",
      "\n",
      "../data/processed/46- Unit of Activity (UoA)_page_1_Text.md\n",
      "../data/processed/46- Unit of Activity (UoA)_page_2_Text.md\n"
     ]
    }
   ],
   "source": [
    "texts = [d for d in docs if d.endswith(\".md\") and \"_Text\" in d]\n",
    "print(f\"Found {len(texts)} text files.\")\n",
    "print(f\"\\nTexts:\\n\")\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3ffdb349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Unit of Activity (UoA)\n",
      "\n",
      "## OBJECTIVE\n",
      "\n",
      "## WELL CONSTRUCTION UNIT OF ACTIVITY LEVELS\n",
      "\n",
      "## SUB-BUSINESS LINE UNIT OF ACTIVITY MATRIX\n",
      "\n",
      "## Unit of Activity (UoA)\n",
      "\n",
      "- · WELL CONSTRUCTION UNIT OF ACTIVITY LEVELS\n",
      "- · SUB-BUSINESS LINE UNIT OF ACTIVITY MATRIX\n",
      "- · HOW TO FIND UNIT OF ACTIVITY\n",
      "- · REGIONAL BUSINESS ACTIVITY CAPTURE GUIDELINES (Manual Entry in Sharepoint)\n",
      "\n",
      "The objective of this page is to describe the rules used to build the Well Construction Unit of Activity (UoA) and, subsequently, the Well Construction Unit of Activity dashboard that can be found under https://biportfolio.data.slb.com/ in the PSD section.\n",
      "\n",
      "Capturing activity in terms of job count, historical data, and forecast is critical for understanding the performance and trends of the Well Construction Business.  It allows for accurate monitoring of progress and identification of potential issues, as well as the ability to make informed decisions about future operations. The accuracy of forecasting depends on the quality of the data. Inaccurate or incomplete data can lead to inaccurate forecasts and poor decision-making.\n",
      "\n",
      "Therefore, it is essential to ensure the data is accurate, complete, and current before forecasting.\n",
      "\n",
      "There are three levels of Units of Activity in Well Construction - Division, Business Line, and Sub-Business Line.\n"
     ]
    }
   ],
   "source": [
    "with open(texts[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "fe440f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model instance\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",      # The Ollama model name\n",
    "    base_url=\"http://localhost:11434\",  # Default Ollama server address\n",
    "    # embed_instruction=\"passage: \"   # (Optional) prefix if your model expects an instruction\n",
    ")\n",
    "\n",
    "buffer_size = 5\n",
    "breakpoint_threshold_type = \"percentile\"\n",
    "breakpoint_threshold_amount = 0.95\n",
    "sentence_split_regex =  r\"(?<=[.!?]) +\" # r\"\\n\\n\\n\"  # \n",
    "max_chunk_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a70d71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splitter = SemanticChunker(\n",
    "    embedding_model,\n",
    "    buffer_size=buffer_size,\n",
    "    breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "    breakpoint_threshold_amount=breakpoint_threshold_amount,\n",
    "    sentence_split_regex=sentence_split_regex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7f3c02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing step 1 with 1 segments in the queue.\n",
      "================================================\n",
      "\n",
      "Current segment: ## Unit of... (length: 278 tokens)\n",
      "\n",
      "Segment exceeds max chunk size of 64 tokens (has 278).\n",
      "\n",
      "Processing step 2 with 1 segments in the queue.\n",
      "================================================\n",
      "\n",
      "Current segment: ## Unit of... (length: 277 tokens)\n",
      "\n",
      "Segment exceeds max chunk size of 64 tokens (has 277).\n",
      "\n",
      "No semantic chunks created; using hard split fallback.\n",
      "  - Adding fallback chunk (tokens: 64): ## Unit of Activity (UoA) ## O...\n",
      "  - Adding fallback chunk (tokens: 64): MATRIX - · HOW TO FIND UNIT OF...\n",
      "  - Adding fallback chunk (tokens: 64): that can be found under https:...\n",
      "  - Adding fallback chunk (tokens: 64): ability to make informed decis...\n",
      "  - Adding fallback chunk (tokens: 15): in Well Construction - Divisio...\n"
     ]
    }
   ],
   "source": [
    "semntic_chunks = process_semantic_split([text], semantic_splitter, max_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3c11984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Chunk 1:\n",
      "## Unit of Activity (UoA) ## OBJECTIVE ## WELL CONSTRUCTION UNIT OF ACTIVITY LEVELS ## SUB-BUSINESS LINE UNIT OF ACTIVITY MATRIX ## Unit of Activity (UoA) - · WELL CONSTRUCTION UNIT OF ACTIVITY LEVELS - · SUB-BUSINESS LINE UNIT OF ACTIVITY\n",
      "\n",
      "Token count: 64\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Semantic Chunk 2:\n",
      "MATRIX - · HOW TO FIND UNIT OF ACTIVITY - · REGIONAL BUSINESS ACTIVITY CAPTURE GUIDELINES (Manual Entry in Sharepoint) The objective of this page is to describe the rules used to build the Well Construction Unit of Activity (UoA) and, subsequently, the Well Construction Unit of Activity dashboard\n",
      "\n",
      "Token count: 64\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Semantic Chunk 3:\n",
      "that can be found under https://biportfolio.data.slb.com/ in the PSD section. Capturing activity in terms of job count, historical data, and forecast is critical for understanding the performance and trends of the Well Construction Business. It allows for accurate monitoring of progress and identification of potential issues, as well as the\n",
      "\n",
      "Token count: 64\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Semantic Chunk 4:\n",
      "ability to make informed decisions about future operations. The accuracy of forecasting depends on the quality of the data. Inaccurate or incomplete data can lead to inaccurate forecasts and poor decision-making. Therefore, it is essential to ensure the data is accurate, complete, and current before forecasting. There are three levels of Units of Activity\n",
      "\n",
      "Token count: 64\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Semantic Chunk 5:\n",
      "in Well Construction - Division, Business Line, and Sub-Business Line.\n",
      "\n",
      "Token count: 15\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(semntic_chunks, 1):\n",
    "    print(f\"Semantic Chunk {idx}:\\n{chunk}\")\n",
    "    print(f\"\\nToken count: {get_token_count(chunk)}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5168a",
   "metadata": {},
   "source": [
    "### Calculate Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d89941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.embeddings(\n",
    "    model='mxbai-embed-large',\n",
    "    prompt='Your document text here'\n",
    ")\n",
    "\n",
    "embed_vector = response['embedding']  # list of ~1,024 floats\n",
    "print(len(embed_vector))  # ➝ 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806c40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    SimpleKeywordTableIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eee8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee934e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.http.exceptions import (\n",
    "    ResponseHandlingException,\n",
    "    UnexpectedResponse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091102ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.base import BaseIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fda1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c3368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "ollama_embed = OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large\",\n",
    "    base_url=\"http://localhost:11434\" ,\n",
    "    ollama_additional_kwargs={},   # e.g. {\"mirostat\": 0}\n",
    "    client_kwargs=None,            # optional httpx.Client params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469b22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "test_input = \"This is a test input to determine embedding dimensions.\"\n",
    "# OllamaEmbedding uses the method 'get_text_embedding' to get the embedding vector\n",
    "embed_vector = ollama_embed.get_text_embedding(test_input)\n",
    "embed_dim = len(embed_vector)\n",
    "print(f\"Embedding dimension: {embed_dim}\")  # Should print 1024 for mxbai-embed-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b631a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/46- Unit of Activity (UoA)_page_1_Text.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_3_Text.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_2_Text.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_1_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_2_Picture_2_description.txt\n",
      "../data/processed/46- Unit of Activity (UoA)_page_2_Text.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_1_description.txt\n",
      "../data/processed/46- Unit of Activity (UoA)_page_1_Table_2.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_3_Picture_1_description.txt\n",
      "../data/processed/46- Unit of Activity (UoA)_page_2_Table_1.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Text.md\n",
      "../data/processed/46- Unit of Activity (UoA)_page_1_Table_1.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Text.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_2_Picture_1_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_3_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_5_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_3_Picture_2_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Table_4.md\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_3_description.txt\n",
      "../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_2_description.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DOCS_PATH = \"../data/processed\"\n",
    "docs = [os.path.join(DOCS_PATH, d) for d in os.listdir(DOCS_PATH)]\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60b5af5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_2_description.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_2_description.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     description = f.read()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(description)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projets perso/multimodal-document-extraction/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_2_description.txt'"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "    \"../data/processed/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_2_description.txt\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\"\n",
    ") as f:\n",
    "    description = f.read()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d48b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_markdown_tables(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds all Markdown tables in the text.\n",
    "    A Markdown table is defined as:\n",
    "      1) A header row    (a line starting and ending with '|')\n",
    "      2) A separator row (a line of pipes, dashes, spaces, or colons)\n",
    "      3) One or more data rows (lines starting and ending with '|')\n",
    "\n",
    "    The tables are expected to be in the format:\n",
    "\n",
    "    | Column1 | Column2 |\n",
    "    |---------|---------|\n",
    "    | Data1   | Data2   |\n",
    "    \n",
    "    If no tables are found, it returns an empty list.\n",
    "\n",
    "    Returns a list of the raw table strings.\n",
    "    \"\"\"\n",
    "    table_pattern = (\n",
    "        r'(?:^[ \\t]*\\|.*\\r?\\n)'                    # header row\n",
    "        r'(?:^[ \\t]*\\|[-\\s|:]+\\r?\\n)'              # separator row\n",
    "        r'(?:^[ \\t]*\\|.*\\r?\\n?)+'                  # one or more data rows\n",
    "    )\n",
    "    return re.findall(table_pattern, text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_markdown_tables(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes all Markdown tables (as defined above) from the text.\n",
    "    \"\"\"\n",
    "    table_pattern = (\n",
    "        r'(?:^[ \\t]*\\|.*\\r?\\n)'\n",
    "        r'(?:^[ \\t]*\\|[-\\s|:]+\\r?\\n)'\n",
    "        r'(?:^[ \\t]*\\|.*\\r?\\n?)+'\n",
    "    )\n",
    "    return re.sub(table_pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_code_blocks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strips out any fenced code block (```...```), regardless of language.\n",
    "    \"\"\"\n",
    "    return re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "def remove_mermaid_blocks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Specifically strips out any ```mermaid ... ``` block.\n",
    "    \"\"\"\n",
    "    return re.sub(r'```mermaid.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "def remove_extracted_text_blocks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any ```EXTRACTED TEXT ... ``` sections.\n",
    "    \"\"\"\n",
    "    return re.sub(r'```EXTRACTED TEXT.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "def clean_up_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up the input text by:\n",
    "      1. Removing all fenced code blocks\n",
    "      2. Removing mermaid diagrams\n",
    "      3. Removing EXTRACTED TEXT sections\n",
    "      4. Removing all Markdown tables\n",
    "    \"\"\"\n",
    "    text = remove_code_blocks(text)\n",
    "    text = remove_mermaid_blocks(text)\n",
    "    # text = remove_extracted_text_blocks(text)\n",
    "    text = remove_markdown_tables(text)\n",
    "    return text\n",
    "\n",
    "# def extract_markdown_table(s):\n",
    "#     \"\"\"\n",
    "#     Extracts Markdown tables from a given string.\n",
    "#     This function uses a regular expression to find all Markdown tables in the input string.\n",
    "#     The tables are expected to be in the format:\n",
    "#     | Column1 | Column2 |\n",
    "#     |---------|---------|\n",
    "#     | Data1   | Data2   |\n",
    "#     If no tables are found, it returns an empty list.\n",
    "\n",
    "#     Args:\n",
    "#         - s (str): The input string that may contain Markdown tables.\n",
    "\n",
    "#     Returns:\n",
    "#         - List[str]: A list of strings, each representing a Markdown table found in the input string.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         table_pattern = r\"(\\|.*\\|\\s*\\n\\|[-| ]+\\|\\s*\\n(\\|.*\\|\\s*\\n)+)\"\n",
    "#         tables = re.findall(table_pattern, s, re.MULTILINE)\n",
    "#     except:\n",
    "#         tables = []\n",
    "#         print(\"Error extracting markdown tables\", f\"Error extracting markdown tables from text '{s[:50]}'.\")  # logc(\"Error extracting markdown tables\", f\"Error extracting markdown tables from text '{s[:50]}'.\")\n",
    "\n",
    "#     return tables\n",
    "\n",
    "# def remove_code(s):\n",
    "#     return re.sub(r\"```python(.*?)```\", \"\", s, flags=re.DOTALL)\n",
    "\n",
    "# def remove_markdown(s):\n",
    "#     return re.sub(r\"```markdown(.*?)```\", \"\", s, flags=re.DOTALL)\n",
    "\n",
    "# def remove_mermaid(s):\n",
    "#     return re.sub(r\"```mermaid(.*?)```\", \"\", s, flags=re.DOTALL)\n",
    "\n",
    "# def remove_extracted_text(s):\n",
    "#     return re.sub(r\"```EXTRACTED TEXT(.*?)```\", \"\", s, flags=re.DOTALL)\n",
    "\n",
    "# def clean_up_text(text):\n",
    "#     \"\"\"\n",
    "#     Cleans up the input text by removing code blocks, mermaid diagrams, markdown formatting (tables, images, etc.),\n",
    "#     and any extracted text sections. This is useful for preparing text for further processing\n",
    "#     or analysis, ensuring that the text is free from unnecessary formatting and code snippets.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): The input text to be cleaned up.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The cleaned-up text with all specified elements removed.\n",
    "#     \"\"\"\n",
    "#     # Remove code blocks, mermaid diagrams, markdown formatting, and extracted text sections\n",
    "#     tables = extract_markdown_table(text)\n",
    "#     if tables:\n",
    "#         for table in tables:\n",
    "#             text = text.replace(table[0], \"\")\n",
    "#     text = remove_code(text)\n",
    "#     text = remove_markdown(text)\n",
    "#     # text = remove_extracted_text(text)\n",
    "#     # text = remove_mermaid(text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1bef561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image provided is a screenshot of a pegging report in a business intelligence (BI) dashboard. The report is designed to analyze and manage the relationship between demand and supply within a supply chain management system. The document is structured to provide insights into the availability and allocation of materials and resources.\n",
      "\n",
      "### 1. Information Conveyed by the Image:\n",
      "The image outlines a pegging report that links demand and supply. It provides a detailed view of the supply and demand status, including open reservations, stock on hand (SOH), and planned orders. The report is refreshed daily and is used to manage and execute supply chain operations efficiently.\n",
      "\n",
      "### 2. Description of the Image:\n",
      "The image is a screenshot of a BI dashboard with a blue header and footer. The main content is divided into sections: a scope description, a data table, and a summary at the bottom. The scope describes the demand and supply elements, while the data table provides detailed information on open reservations, stock levels, and order statuses.\n",
      "\n",
      "### 3. Description of the Data Table:\n",
      "The data table is titled \"Data Table - Open Reservation - Supply Element Availability Status.\" It contains the following columns:\n",
      "\n",
      "- **Index**: A unique identifier for each row.\n",
      "- **Material-Plant**: The material and plant associated with the reservation.\n",
      "- **Material**: The specific material.\n",
      "- **Reservation-Line**: The line number of the reservation.\n",
      "- **Open Qty - SOH - Total**: The total quantity of the material available in stock.\n",
      "- **Open Qty - PO-LN - Open Qty**: The quantity of the material available in open purchase orders.\n",
      "- **Combined SOH & PO Pegging**: The combined status of the material availability.\n",
      "- **Primary Pegged PO-LN - Order Qty**: The quantity of the material ordered in the primary purchase order.\n",
      "- **Pegged Main PO GR Status**: The status of goods received (GR) for the main purchase order.\n",
      "- **Pegged Main PO Invoice Status**: The status of the invoice for the main purchase order.\n",
      "- **Planned Orders**: The quantity of the material planned to be ordered.\n",
      "- **Maximo Asset**: The asset associated with the material.\n",
      "- **Maximo Serial No**: The serial number of the asset.\n",
      "\n",
      "The table provides a comprehensive view of the supply and demand status, helping to manage and allocate resources effectively.\n",
      "\n",
      "### 4. Markdown Representation of the Data Table:\n",
      "```markdown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_description = clean_up_text(description)\n",
    "print(clean_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36a347ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageStat\n",
    "from pathlib import Path\n",
    "\n",
    "def is_relevant_image(\n",
    "        image_path: Path,\n",
    "        image_pixel_threshold: int,\n",
    "        image_pixel_variance_threshold: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Determines if an image is relevant based on non-white pixel count and color variance.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (Path): Path to the image file.\n",
    "    - pixel_threshold (int): Minimum number of non-white pixels required.\n",
    "    - variance_threshold (float): Minimum average variance across color channels.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the image is relevant, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Calculate non-white pixels in grayscale\n",
    "            grayscale = img.convert(\"L\")\n",
    "            histogram = grayscale.histogram()\n",
    "            non_white = sum(histogram[:-1])\n",
    "\n",
    "            if non_white <= image_pixel_threshold:\n",
    "                return False\n",
    "\n",
    "            # Calculate average variance across all color channels\n",
    "            stat = ImageStat.Stat(img)\n",
    "            avg_variance = sum(stat.var) / len(stat.var)\n",
    "\n",
    "            if avg_variance < image_pixel_variance_threshold:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "    except (IOError, OSError) as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf835487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_5.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_2_Picture_1.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_2_Picture_2.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_3.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_2.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_1_Picture_1.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_3.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_2.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_4_Picture_1.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_3_Picture_1.jpg\n",
      "../data/detections/172- Demand&Supply Visualization - 5.2 Soft Pegging_page_3_Picture_2.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DOCS_PATH = \"../data/detections\"\n",
    "images = [os.path.join(DOCS_PATH, d) for d in os.listdir(DOCS_PATH)]\n",
    "images = [img for img in images if \"_Picture_\" in img]\n",
    "\n",
    "for img in images:\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05e8bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "IMAGE_PIXEL_THRESHOLD = 10000\n",
    "IMAGE_PIXEL_VARIANCE_THRESHOLD = 500\n",
    "\n",
    "for img in images:\n",
    "    print(\n",
    "        is_relevant_image(\n",
    "        img,\n",
    "        IMAGE_PIXEL_THRESHOLD,\n",
    "        IMAGE_PIXEL_VARIANCE_THRESHOLD\n",
    "        )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098c259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
