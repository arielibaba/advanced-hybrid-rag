{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Web App\n",
    "\n",
    "This notebooks allows to run the code to set up and launch the RC Assist WebApp.\n",
    "\n",
    "The following tasks are performed:\n",
    "\n",
    "- Load of the child and parent documents indexes into memory (for faster inference)\n",
    "- Build the retrieval system to get the relevant context to answer the queries\n",
    "- Build several components to enhance the chatbot: query reformulation/expansion, memory management, context refining/reranking\n",
    "- Add response streaming capability to the chatbot\n",
    "\n",
    "Once the code executed, the web application will expose the chatbot web interface via a URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ariel/projects/multimodal-rag-with-evaluation-1/.rag_v1_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from src.constants import (\n",
    "    INDEX_PATH,\n",
    "    CHILD_DOCUMENTS_INDEX,\n",
    "    PARENT_DOCUMENTS_INDEX,\n",
    "    TOP_K_RETRIEVED_CHILDREN,\n",
    "    TOP_K_RERANKED_PARENTS,\n",
    "    TOP_K_REFS,\n",
    "    MEMORY_WINDOW,\n",
    "    TEMPERATURE_QUERY_EXPANSION,\n",
    "    TEMPERATURE_RESPONSE \n",
    ")\n",
    "from src.doc_utils import retrieve_from_keyword_index\n",
    "from src.utils.text_utils import get_token_count\n",
    "\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Config\n",
    "model_info = {\n",
    "    \"AZURE_OPENAI_RESOURCE\": os.environ.get(\"AZURE_OPENAI_RESOURCE\"),\n",
    "    \"AZURE_OPENAI_KEY\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    \"AZURE_OPENAI_MODEL_VISION\": os.environ.get(\"AZURE_OPENAI_MODEL_VISION\"),\n",
    "    \"AZURE_OPENAI_MODEL\": os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "    \"AZURE_OPENAI_API_VERSION\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "embed_model_info = {\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE\": os.environ.get(\"AZURE_OPENAI_RESOURCE\"),\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE_KEY\": os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE_KEY\"),\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL\": os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL_API_VERSION\": os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Main LLM\n",
    "model = AzureOpenAI(\n",
    "    model=model_info[\"AZURE_OPENAI_MODEL\"],\n",
    "    deployment_name=model_info[\"AZURE_OPENAI_MODEL\"],\n",
    "    api_key=model_info[\"AZURE_OPENAI_KEY\"],\n",
    "    azure_endpoint=f\"https://{model_info['AZURE_OPENAI_RESOURCE']}.openai.azure.com/\",\n",
    "    api_version=model_info[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=TEMPERATURE_RESPONSE\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    deployment_name=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    api_key=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE_KEY\"],\n",
    "    azure_endpoint=f\"https://{embed_model_info['AZURE_OPENAI_EMBEDDING_MODEL_RESOURCE']}.openai.azure.com/\",\n",
    "    api_version=embed_model_info[\"AZURE_OPENAI_EMBEDDING_MODEL_API_VERSION\"]\n",
    ")\n",
    "\n",
    "# LLM for Query Expansion\n",
    "query_expansion_model = AzureOpenAI(\n",
    "    model=model_info[\"AZURE_OPENAI_MODEL\"],\n",
    "    deployment_name=model_info[\"AZURE_OPENAI_MODEL\"],\n",
    "    api_key=model_info[\"AZURE_OPENAI_KEY\"],\n",
    "    azure_endpoint=f\"https://{model_info['AZURE_OPENAI_RESOURCE']}.openai.azure.com/\",\n",
    "    api_version=model_info[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=TEMPERATURE_QUERY_EXPANSION\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the child and parent documents indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index %s loaded successfully! .child_documents\n",
      "Index %s loaded successfully! .parent_documents\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    child_index_path = Path(INDEX_PATH) / CHILD_DOCUMENTS_INDEX\n",
    "    children_storage_context = StorageContext.from_defaults(persist_dir=child_index_path)\n",
    "    child_documents_index = load_index_from_storage(children_storage_context)\n",
    "    print(\"Index %s loaded successfully!\", child_index_path.stem)\n",
    "except (IOError, OSError) as e:\n",
    "    print(\n",
    "        \"Failed to load index %s. The following error occurred:\\n%s\", child_index_path.stem, e\n",
    "    )\n",
    "\n",
    "try:\n",
    "    parent_index_path = Path(INDEX_PATH) / PARENT_DOCUMENTS_INDEX\n",
    "    parents_storage_context = StorageContext.from_defaults(persist_dir=parent_index_path)\n",
    "    parent_documents_index = load_index_from_storage(parents_storage_context)\n",
    "    print(\"Index %s loaded successfully!\", parent_index_path.stem)\n",
    "except (IOError, OSError) as e:\n",
    "    print(\n",
    "        \"Failed to load index %s. The following error occurred:\\n%s\", parent_index_path.stem, e\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Retiever and Reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child Documents Retriever\n",
    "child_documents_retriever = VectorIndexRetriever(\n",
    "    index=child_documents_index,\n",
    "    similarity_top_k=TOP_K_RETRIEVED_CHILDREN\n",
    ")\n",
    "\n",
    "# Reranker (to rank the relevant documents by descending similarity score)\n",
    "reranker = LLMRerank(\n",
    "    choice_batch_size=5,\n",
    "    top_n=TOP_K_RERANKED_PARENTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 1) Token-limit tools\n",
    "#######################################################################\n",
    "def limit_chat_history(history, max_tokens=MEMORY_WINDOW):\n",
    "    \"\"\"\n",
    "    Truncate conversation history from the front if the total tokens exceed max_tokens.\n",
    "    Each message in the history is expected to be a dictionary with keys \"role\" and \"content\".\n",
    "    The function iterates backwards (i.e. from the most recent messages) and accumulates messages\n",
    "    until adding another would exceed max_tokens, then returns the surviving messages in the correct order.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    truncated_history = []\n",
    "    for message in reversed(history):\n",
    "        msg_tokens = get_token_count(message[\"content\"])\n",
    "        if total_tokens + msg_tokens > max_tokens:\n",
    "            break\n",
    "        truncated_history.append(message)\n",
    "        total_tokens += msg_tokens\n",
    "    truncated_history.reverse()  # Restore chronological order.\n",
    "    return truncated_history\n",
    "\n",
    "###################\n",
    "# 2) Streaming generator\n",
    "###################\n",
    "def stream_llm_output(llm, messages):\n",
    "    \"\"\"Yields tokens from llm.stream_chat(messages).\"\"\"\n",
    "    for token in llm.stream_chat(messages):\n",
    "        yield token.delta\n",
    "\n",
    "def run_streaming(llm, messages):\n",
    "    \"\"\"Accumulate tokens into a response, yielding partial outputs.\"\"\"\n",
    "    response = \"\"\n",
    "    for token in stream_llm_output(llm, messages):\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "###################\n",
    "# 3) Rewrite Query\n",
    "###################\n",
    "def rewrite_query(user_query: str, conversation_history_str: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Enhanced rewrite of the user's query to:\n",
    "      1) Identify multiple sub-questions (if any).\n",
    "      2) Rewrite each sub-question as a stand-alone query, incorporating any\n",
    "         context from the previous conversation (e.g., clarifying pronouns).\n",
    "      3) Return them as bullet points, each bullet containing a rewritten sub-question.\n",
    "    \n",
    "    The final string might have multiple lines if multiple sub-questions exist.\n",
    "    If there's only one question, returns a single bullet with that question.\n",
    "\n",
    "    Incorporates examples (BERT, Bill Clinton) to demonstrate referencing prior context.\n",
    "    \"\"\"\n",
    "\n",
    "    # SYSTEM MESSAGE: instruct the LLM on how to rewrite\n",
    "    system_message = (\n",
    "        \"You are able to reason from previous conversation and the recent question, \"\n",
    "        \"to come up with a rewrite that is self-contained.\\n\"\n",
    "        \"Given the conversation and the user's new query:\\n\"\n",
    "        \"1) Identify multiple sub-questions (if any).\\n\"\n",
    "        \"2) Rewrite each sub-question as a stand-alone query, incorporating any context from the previous conversation (e.g., clarifying pronouns).\\n\"\n",
    "        \"3) Return them as bullet points, each bullet containing a rewritten sub-question.\\n\\n\"\n",
    "        \"A few examples:\\n\\n\"\n",
    "        \"# Example 1\\n\"\n",
    "        \"## Previous conversation\\n\"\n",
    "        \"user: Who is Bill Clinton?\\n\"\n",
    "        \"assistant: Bill Clinton is an American politician who served as the 42nd President of the United States.\\n\"\n",
    "        \"## New question\\n\"\n",
    "        \"user: When was he born?\\n\"\n",
    "        \"## Rewritten question\\n\"\n",
    "        \"- When was Bill Clinton born?\\n\\n\"\n",
    "        \"# Example 2\\n\"\n",
    "        \"## Previous conversation\\n\"\n",
    "        \"user: What is BERT?\\n\"\n",
    "        \"assistant: BERT stands for \\\"Bidirectional Encoder Representations from Transformers.\\\" \"\n",
    "        \"It is a natural language processing (NLP) model developed by Google.\\n\"\n",
    "        \"user: What data was used for its training?\\n\"\n",
    "        \"assistant: The BERT model was trained on a large corpus of publicly available text from the internet.\\n\"\n",
    "        \"## New question\\n\"\n",
    "        \"user: How else can I apply it?\\n\"\n",
    "        \"## Rewritten question\\n\"\n",
    "        \"- How can I apply the BERT model to other tasks?\\n\"\n",
    "        \"Example 3:\\n\"\n",
    "        \"User query: 'How much will the temperature rise by 2100 and what are the main causes?'\\n\"\n",
    "        \"Rewrite:\\n\"\n",
    "        \"- How much will the temperature rise by 2100?\\n\"\n",
    "        \"- What are the main causes of that temperature rise?\\n\\n\"\n",
    "        \"If there is only one question, provide one bullet. If there are multiple sub-questions, each should be a separate bullet.\\n\"\n",
    "    )\n",
    "\n",
    "    # USER PROMPT: feed the conversation and new query\n",
    "    user_prompt = f\"\"\"\n",
    "    ## Previous conversation\n",
    "    {conversation_history_str}\n",
    "\n",
    "    ## New question\n",
    "    {user_query}\n",
    "\n",
    "    ## Rewritten question (please list bullets if multiple)\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_message),\n",
    "        ChatMessage(role=\"user\", content=user_prompt),\n",
    "    ]\n",
    "\n",
    "    # Use the same query_expansion_model (or whichever LLM you want) to perform rewriting\n",
    "    response = query_expansion_model.chat(messages)\n",
    "    rewritten = response.message.content.strip()\n",
    "\n",
    "    # Fallback if the model returned nothing\n",
    "    if not rewritten:\n",
    "        return f\"- {user_query}\"\n",
    "\n",
    "    return rewritten\n",
    "\n",
    "###################\n",
    "# 4) Query Expansion\n",
    "###################\n",
    "def parse_expanded_queries(response_text: str, max_queries: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse the LLM output to extract candidate expanded queries from the section\n",
    "    labeled \"Step 3 - Expanded Queries:\" and return up to 'max_queries' queries.\n",
    "    Expected format in LLM output (for example):\n",
    "        Step 3 - Expanded Queries:\n",
    "        - query 1\n",
    "        - query 2\n",
    "        ...\n",
    "    If nothing is found, returns an empty list.\n",
    "    \"\"\"\n",
    "    marker = \"Step 3\"\n",
    "    start = response_text.find(marker)\n",
    "    if start == -1:\n",
    "        return []\n",
    "    substring = response_text[start:]\n",
    "    lines = substring.splitlines()\n",
    "    candidate_queries = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        if \"Expanded Queries:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:\n",
    "                if stripped_line.startswith(\"-\"):\n",
    "                    stripped_line = stripped_line.lstrip(\"- \").strip()\n",
    "                candidate_queries.append(stripped_line)\n",
    "    return candidate_queries[:max_queries]\n",
    "\n",
    "def expand_query(user_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates expansions for each sub-question found in the user's rewritten query.\n",
    "    1) Parses the bullet points from 'user_query' (which may contain multiple sub-questions).\n",
    "    2) For each sub-question, we call the LLM to produce expanded variants or synonyms.\n",
    "    3) Returns a combined list of expansions from all sub-questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split 'user_query' by lines/bullets. We assume each bullet starts with '-' or '*'.\n",
    "    sub_questions = []\n",
    "    for line in user_query.splitlines():\n",
    "        # Trim and check if it starts with a bullet\n",
    "        line_strip = line.strip(\"-* \\t\")\n",
    "        if line_strip:\n",
    "            sub_questions.append(line_strip)\n",
    "\n",
    "    # If nothing parsed, treat the entire user_query as one question\n",
    "    if not sub_questions:\n",
    "        sub_questions = [user_query]\n",
    "\n",
    "    all_expanded = []\n",
    "\n",
    "    # We'll reuse the same system prompt structure for each sub-question,\n",
    "    # but you can modify this as needed.\n",
    "    system_message_template = (\n",
    "        \"You are an expert assistant in query expansion. Your task:\\n\"\n",
    "        \"1. Take the sub-question provided.\\n\"\n",
    "        \"2. Identify synonyms or semantically related terms.\\n\"\n",
    "        \"3. Return final expansions under 'Step 3 - Expanded Queries:' as a bullet list.\\n\"\n",
    "        \"Do not add external info. Use only the sub-question.\"\n",
    "    )\n",
    "\n",
    "    for sq in sub_questions:\n",
    "        user_prompt = f\"\"\"\n",
    "        Sub-question to expand:\n",
    "        \"{sq}\"\n",
    "\n",
    "        Instructions:\n",
    "        - Identify sub-questions if any remain.\n",
    "        - Expand each key concept with synonyms or related terms.\n",
    "        - Write them under \"Step 3 - Expanded Queries:\" as a concise bullet list.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_message_template),\n",
    "            ChatMessage(role=\"user\", content=user_prompt),\n",
    "        ]\n",
    "\n",
    "        response = query_expansion_model.chat(messages)\n",
    "        response_text = response.message.content\n",
    "\n",
    "        # Parse expansions from this single sub-question\n",
    "        candidate_queries = parse_expanded_queries(response_text, max_queries=5)\n",
    "\n",
    "        # If no expansions are found, fallback to original\n",
    "        if not candidate_queries:\n",
    "            candidate_queries = [sq]\n",
    "\n",
    "        # Collect expansions\n",
    "        all_expanded.extend(candidate_queries)\n",
    "\n",
    "    return all_expanded\n",
    "\n",
    "###################\n",
    "# Helper: Convert internal history to Chatbot tuples for Gradio interface\n",
    "###################\n",
    "def convert_history_to_tuples(history: List[dict]) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Converts our internal conversation history (a list of dictionaries) into\n",
    "    the format expected by Gradio Chatbot: a list of (user, assistant) tuples.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    temp = None\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            temp = msg[\"content\"]\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            # If a user message exists, pair it with the assistant message.\n",
    "            if temp is not None:\n",
    "                output.append((temp, msg[\"content\"]))\n",
    "                temp = None\n",
    "            else:\n",
    "                output.append((\"\", msg[\"content\"]))\n",
    "    # In case the last message is from the user without a response.\n",
    "    if temp is not None:\n",
    "        output.append((temp, \"\"))\n",
    "    return output\n",
    "\n",
    "###################\n",
    "# 5) Main conversation function\n",
    "###################\n",
    "def chat_conversation(user_message, history):\n",
    "    \"\"\"\n",
    "    Orchestrates the chat conversation by:\n",
    "      - Maintaining a conversation history (as a list of dictionaries).\n",
    "      - Expanding the user's query.\n",
    "      - Retrieving, de-duplicating, and re-ranking relevant documents.\n",
    "      - Constructing a final prompt with conversation history and retrieved context.\n",
    "      - Streaming the LLM response and appending document references.\n",
    "    The output is converted to a list of (user, assistant) tuples as required by the Gradio Chatbot.\n",
    "    \"\"\"\n",
    "    # If history comes in as tuples, convert it to our internal dictionary format.\n",
    "    if history is None:\n",
    "        history = []\n",
    "    elif len(history) > 0 and isinstance(history[0], (list, tuple)):\n",
    "        new_history = []\n",
    "        for tup in history:\n",
    "            if isinstance(tup, (list, tuple)) and len(tup) == 2:\n",
    "                new_history.append({\"role\": \"user\", \"content\": tup[0]})\n",
    "                new_history.append({\"role\": \"assistant\", \"content\": tup[1]})\n",
    "            else:\n",
    "                new_history.append(tup)\n",
    "        history = new_history\n",
    "\n",
    "    # Append the user's message.\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    # Limit the history based on the token budget.\n",
    "    history = limit_chat_history(history, max_tokens=MEMORY_WINDOW)\n",
    "    # Build a textual summary for query expansion.\n",
    "    conversation_history_str = \"\"\n",
    "    for msg in history:\n",
    "        conversation_history_str += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
    "\n",
    "    # Rewrite the user's query.\n",
    "    rewritten_query = rewrite_query(\n",
    "        user_query=user_message,\n",
    "        conversation_history_str=conversation_history_str\n",
    "    )\n",
    "\n",
    "    # Expand the user's query.\n",
    "    candidate_queries = expand_query(\n",
    "        user_query=rewritten_query\n",
    "    )\n",
    "    print(f\"Candidate Expanded Queries: {candidate_queries}\")\n",
    "    # Retrieve child nodes for each candidate query.\n",
    "    retrieved_child_nodes = []\n",
    "    for cq in candidate_queries:\n",
    "        retrieved_child_nodes.extend(child_documents_retriever.retrieve(cq))\n",
    "    # Retrieve parent documents and de-duplicate.\n",
    "    raw_parents = []\n",
    "    for node in retrieved_child_nodes:\n",
    "        parent_node = retrieve_from_keyword_index(\n",
    "            index=parent_documents_index,\n",
    "            key=\"name\",\n",
    "            value=node.metadata[\"parent\"]\n",
    "        )\n",
    "        if parent_node:\n",
    "            raw_parents.append(parent_node[0])\n",
    "    unique_parents = []\n",
    "    seen_names = set()\n",
    "    for p in raw_parents:\n",
    "        name = p.metadata.get(\"name\")\n",
    "        if name not in seen_names:\n",
    "            unique_parents.append(p)\n",
    "            seen_names.add(name)\n",
    "    # Rerank the parent documents.\n",
    "    parents_with_score = [NodeWithScore(node=p, score=0.0) for p in unique_parents]\n",
    "    combined_query_str = user_message + \" | \" + \" | \".join(candidate_queries)        # alternatively, if we can rerank according to the user_message. This way we will display higher relevance scores for the reference docs. But, we will not be covering all the aspects of the question (sub-queries).\n",
    "    reranked_parents_with_score = reranker.postprocess_nodes(\n",
    "        parents_with_score,\n",
    "        query_str=combined_query_str\n",
    "    )\n",
    "    reranked_parents = [nws.node for nws in reranked_parents_with_score]\n",
    "\n",
    "    # Build the retrieved context and references strings.\n",
    "    retrieved_context = \"\\n\".join(node.text for node in reranked_parents)\n",
    "    \n",
    "    # Build the references to display the final 'score' (relevance)\n",
    "    unique_ref_node_with_score = []\n",
    "    seen_doc_page = set()\n",
    "\n",
    "    for nws in reranked_parents_with_score:\n",
    "        # Retrieve the nested metadata\n",
    "        source_dict = nws.node.metadata.get(\"source\", {})\n",
    "        doc_name = source_dict.get(\"document\")\n",
    "        page_num = source_dict.get(\"page\")\n",
    "\n",
    "        # Check if this (document, page) was seen before\n",
    "        if (doc_name, page_num) not in seen_doc_page:\n",
    "            unique_ref_node_with_score.append(nws)\n",
    "            seen_doc_page.add((doc_name, page_num))\n",
    "\n",
    "    # Now build the references from the unique NodeWithScore objects, \n",
    "    # slicing to your desired TOP_K_REFS if needed.\n",
    "    references = \"\\n\\n\\n\".join(\n",
    "        f\"{idx}.   {nws.node.metadata.get('source', {}).get('document')}   -   \"\n",
    "        f\"Page {nws.node.metadata.get('source', {}).get('page')}\\n\"\n",
    "        # f\"Relevance:   {int(nws.score)}%\"\n",
    "        for idx, nws in enumerate(unique_ref_node_with_score[:TOP_K_REFS], 1)\n",
    "    )\n",
    "\n",
    "    # Build a conversation summary (exclude the last user message).\n",
    "    conversation_summary_str = \"\"\n",
    "    for message in history[:-1]:\n",
    "        conversation_summary_str += f\"{message['role'].capitalize()}: {message['content']}\\n\"\n",
    "    current_user_question = history[-1][\"content\"]\n",
    "    # Construct the final system and user prompts.\n",
    "    system_message = (\n",
    "        \"You are a helpful and factual assistant. Provide clear, concise, and accurate answers \"\n",
    "        \"based solely on the provided context and conversation history. If the context and \"\n",
    "        \"history do not include sufficient information, state 'No relevant details are available.' \"\n",
    "        \"Do not invent or fabricate information, and do not refer explicitly to the retrieval process.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        \"Below are the conversation history, the user's current question, and the retrieved context \"\n",
    "        \"from our knowledge base. The retrieved context may include relevant text passages, tables \"\n",
    "        \"in Markdown format, or image descriptions.\\n\\n\"\n",
    "        f\"Conversation History:\\n{conversation_summary_str}\\n\\n\"\n",
    "        f\"User Question:\\n{current_user_question}\\n\\n\"\n",
    "        f\"Retrieved Context:\\n{retrieved_context}\\n\\n\"\n",
    "        \"Please provide a clear and integrated answer covering all parts of the user's question. \"\n",
    "        \"If there are multiple sub-questions, address each one. Do not reference the retrieval process or the provided context. \"\n",
    "        \"If there is insufficient information, say: 'No relevant details are available.' \"\n",
    "        \"Respond in English.\"\n",
    "    )\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_message),\n",
    "        ChatMessage(role=\"user\", content=user_prompt),\n",
    "    ]\n",
    "    # Append an empty assistant message before streaming.\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    # Stream the LLM response in partial chunks.\n",
    "    for chunk in run_streaming(model, messages):\n",
    "        history[-1][\"content\"] = chunk\n",
    "        yield convert_history_to_tuples(history)\n",
    "    # Once streaming is complete, append references unless the answer starts with a trigger phrase.\n",
    "    partial_answer = history[-1][\"content\"]\n",
    "    trigger_phrases = (\"no relevant details are available\", \"no information is available.\", \"i'm sorry\", \"i am sorry\", \"sorry\")\n",
    "    if partial_answer.strip().lower().startswith(trigger_phrases):\n",
    "        final_answer = partial_answer\n",
    "    else:\n",
    "        final_answer = partial_answer + \"\\n\\n---\\nReferences:\\n\\n\" + references\n",
    "    history[-1][\"content\"] = final_answer\n",
    "    yield convert_history_to_tuples(history)\n",
    "\n",
    "###################\n",
    "# 6) Reset conversation function\n",
    "###################\n",
    "def reset_conversation():\n",
    "    \"\"\"\n",
    "    Return an empty list for the chatbot (no history)\n",
    "    and an empty string for the user input box.\n",
    "    \"\"\"\n",
    "    return [], \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Chatbot Interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching the RC Chatbot Assistant ...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Expanded Queries: ['What does SAP stand for?', 'What is the meaning of SAP?', 'What is SAP software?', 'What is SAP ERP?', 'What is the purpose of SAP?']\n",
      "Candidate Expanded Queries: ['\"What is Material Requirements Planning (MRP) in the context of SAP ERP?\"', '\"How does Material Requirements Planning (MRP) function in SAP systems?\"', '\"What is the role of MRP in SAP supply chain management?\"', '\"How is MRP used in SAP for production planning and inventory control?\"', '\"What are the features of Material Requirements Planning in SAP software?\"', '\"How does Material Requirements Planning (MRP) function within SAP to handle material planning and inventory management?\"', '\"What is the process of MRP in SAP for managing supply chain planning and stock control?\"', '\"How does SAP\\'s MRP system operate to optimize material procurement and inventory levels?\"', '\"What role does MRP play in SAP for streamlining material scheduling and warehouse management?\"', '\"How does SAP use MRP to support production planning and inventory optimization?\"']\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# 7) Gradio UI\n",
    "###################\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RC Chat Assistant\\n\\nA Gen AI-Powered Solution for Resource Managers, Coordinators & Assistants.\")\n",
    "    chatbot = gr.Chatbot(\n",
    "        height=600,\n",
    "        label=\"RC Chat Assistant\"\n",
    "    )\n",
    "    user = gr.Textbox(\n",
    "        label=\"User\",\n",
    "        show_label=True,\n",
    "        placeholder=\"Message RC Chat Assistant here and press Enter...\",\n",
    "        lines=1,\n",
    "    )\n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    reset_button = gr.Button(\"Reset Conversation\")\n",
    "    # On submit, call chat_conversation (generator) to stream partial answers\n",
    "    submit_button.click(\n",
    "        fn=chat_conversation,\n",
    "        inputs=[user, chatbot],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    # Also allow pressing 'Enter' in the textbox\n",
    "    user.submit(\n",
    "        fn=chat_conversation,\n",
    "        inputs=[user, chatbot],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    # Reset button clears the chatbot and the textbox\n",
    "    reset_button.click(\n",
    "        fn=reset_conversation,\n",
    "        inputs=[],\n",
    "        outputs=[chatbot, user],\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "# Launch the WebApp\n",
    "print(\"Launching the RC Chatbot Assistant ...\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag_v1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
